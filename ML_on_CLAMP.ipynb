{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NaveenPaulBosco/BubbleSort/blob/master/ML_on_CLAMP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2exAi41HJlFr"
      },
      "source": [
        "# Supervised and Unsupervised Machine Learning on CLAMP Dataset\n",
        "### Introduction\n",
        "Malicious programs or malware is an intentionally written program to indulge in various malicious activities, ranging from stealing user’s information to cyber-espionage. The behavioral dynamism exposed by the malware is dependent on various factors such as nature of the attack, sophisticated technology and the rapid increase in exploitable vulnerabilities. Malware attacks also increased along with the rapid growth in the use of digital devices and internet. The exponential increase in the creation of new malware in the last five years has made malware detection a challenging research issue.\n",
        "\n",
        "**NOTE**: This project will be autograded and will require filling out a submission json file. Each section has a python dictionary that you can fill out as you go or you can manually insert answers into the sample json we provide. No credit will be provided even for correct answers in an improperly formatted/key'ed json file so it is probably just easier to build them in the notebook. All answers are expected to be numbers ie 5.1 or strings ie \"feature1\" in the dictionary or json.\n",
        "\n",
        "Good Luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we use an autograder and a seed for this assignment you may run into issues if you dont use the following package versions in your code.\n",
        "\n",
        "**PACKAGE VERSIONS** \n",
        "* pandas==1.3.5\n",
        "* numpy==1.21.6\n",
        "* scikit-learn==1.0.2\n",
        "\n"
      ],
      "metadata": {
        "id": "C6PHAtUbvoJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas==1.3.5 numpy==1.21.6 scikit-learn==1.0.2 joblib==1.1.0"
      ],
      "metadata": {
        "id": "jcvhMz4Zw4s9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZ1irV8WJlFt"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Initialization\n",
        "Run the code cell below to load necessary Python libraries and load the Clamp Malware data. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries necessary for this project\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from time import time\n",
        "from IPython.display import display \n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "pOHsHG6UyT9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_seed_from_email(email_str):\n",
        "  running_total = 0\n",
        "  for char in email_str:\n",
        "    running_total += ord(char)\n",
        "  return running_total\n",
        "## TODO: Change the email below to your gradescope email to create your seed\n",
        "seed = create_seed_from_email(\"nbosco7@gatech.edu\")\n",
        "seed"
      ],
      "metadata": {
        "id": "XROAjiN6pOs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipU24j7mJlFu"
      },
      "outputs": [],
      "source": [
        "# Load the Clamp Data File CLAMP\n",
        "data = pd.read_csv(\"ClaMP_Raw-51842.csv\")\n",
        "data.fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6cmgMDJJlFv"
      },
      "outputs": [],
      "source": [
        "# This displays the top 5 Records\n",
        "display(data.head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2SXHmjgJlFv"
      },
      "outputs": [],
      "source": [
        "# This displays the bottom 5 Records\n",
        "display(data.tail(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** It may be helpful to learn about the Pandas library to succeed on the assignment. \n",
        "\n",
        "The documentation for Pandas can be found at https://pandas.pydata.org/docs/ \n",
        "\n",
        "If you prefer videos, one useful tutorial is the Complete Python Pandas Data Science Tutorial at https://www.youtube.com/watch?v=vmEHCJofslg"
      ],
      "metadata": {
        "id": "FWWx0639s3PW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85JEAbKTJlFw"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Section 1: Data Exploration\n",
        "Our Dataset contains records of information gathered from Malware and Not-Malware Portable Executable (PE) Files. \n",
        "\n",
        "In this section we will do some of the intial exploration of the data (and how it was generated) and begin to create some graphs and summary statistics to understand the dataset better.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beyond just printing out values of records and looking through them by hand it is important to spend time on a step of the process called Exploratory Data Analysis (EDA) in which we create and analyze graphs, summary statistics and other views of the data to understand it better. This usually also means reading documentation about the data to understand how we should use it. We have provided 2 examples here for you (first is a histogram of each feature and second is a more automated report from an opensource library Pandas Profiling)"
      ],
      "metadata": {
        "id": "N8thMIoJxu2t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2ur_QuEJlFv"
      },
      "outputs": [],
      "source": [
        "#Display the distribution of the dataset (Note: that the last column from this dataset, 'Class', will be our target label)\n",
        "columns =data.columns.values.tolist()\n",
        "data[columns].hist(stacked=False, bins=100, figsize=(20,180), layout=(56,2));"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip\n",
        "from pandas_profiling import ProfileReport"
      ],
      "metadata": {
        "id": "pmMhXgljedPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "profile = ProfileReport(data, minimal=True)"
      ],
      "metadata": {
        "id": "NTSjpTtuej3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "profile.to_notebook_iframe()"
      ],
      "metadata": {
        "id": "a-YftQYHel0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The class column contains a 1 if the target is a Malware sample and a 0 if the target is a Not-Malware sample. A cursory investigation of the dataset will determine how many instances fit into either group, and will tell us about the percentage of these instances with malware. In the code cell below, you will need to compute the following:\n",
        "*  The total number of records, `'n_records'`\n",
        "- The number of instances without malware, `'n_non_malware'`.\n",
        "- The number of instances of malware `'n_malware'`.\n",
        "- The percentage of instances with malware, `'greater_percent'`."
      ],
      "metadata": {
        "id": "PyBttAzZypwM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gEFNX1KJlFw"
      },
      "outputs": [],
      "source": [
        "#Total number of records\n",
        "n_records = len(data)\n",
        "#TODO Complete this code: Hint class is the Target\n",
        "\n",
        "#Number of records where malware was not detected\n",
        "n_non_malware = len(data.loc[data['class'] == 0])\n",
        "#TODO Complete this code: Hint class is the Target\n",
        "\n",
        "#Number of records where where malware was detected\n",
        "n_malware =  len(data.loc[data['class'] == 1])\n",
        "#TODO Complete this code: Hint class is the Target\n",
        "\n",
        "# Percentage of instances of malware\n",
        "greater_percent =  (n_malware / n_records) * 100\n",
        "#TODO Complete this code: Hint class is the Target\n",
        "\n",
        "# Print the results\n",
        "print(\"Total number of records: {}\".format(n_records))\n",
        "print(\"The number of records without Malware: {}\".format(n_non_malware))\n",
        "print(\"The number of records with Malware: {}\".format(n_malware))\n",
        "print(\"Percentage of instances where malware is detected: {}%\".format(greater_percent))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "section_1 = {    \n",
        "    \"n_records\": n_records,\n",
        "    \"n_non_malware\": n_non_malware,\n",
        "    \"n_malware\": n_malware,\n",
        "    \"greater_percent\": greater_percent\n",
        "    }"
      ],
      "metadata": {
        "id": "68scy3kB3ETj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "all_sections = {**section_1}\n",
        "with open('submission.json', 'w') as f:\n",
        "     f.write(json.dumps(all_sections))"
      ],
      "metadata": {
        "id": "dh7lBx_QqXeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRHT8_roJlFy"
      },
      "source": [
        "### Check the data types for the dataset.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEqTSqcmJlFy"
      },
      "outputs": [],
      "source": [
        "# Get the data types from the dataset and return a count in unique_data_types\n",
        "unique_data_types = len(data.dtypes.unique())\n",
        "print(\"There are {} unique data types\".format(unique_data_types))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Additional Exploration\n",
        "Typically Additional data exploration through Graphs, Data Statistics and Domain Expertise is done. We wont be grading you on this but if you have extra time feel free to take the time to create plots that help you understand the data source better and consider new features that could be created from one or more of the current features. "
      ],
      "metadata": {
        "id": "8X68OxOD77j9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AgEP_mpJlFx"
      },
      "source": [
        "### **Feature set**     \n",
        "\n",
        "To learn more about how this dataset was created see the github page: https://github.com/urwithajit9/ClaMP. The features for this dataset are extracted from a windows PE file using pefile: https://github.com/erocarrera/pefile\n",
        "\n",
        "You will probably need to do some aditional research outside of what we provide here to understand what a PE file is and what these features relate to.\n",
        "\n",
        "Some Additional Resources:\n",
        "* https://learn.microsoft.com/en-us/windows/win32/debug/pe-format\n",
        "* https://en.wikipedia.org/wiki/Portable_Executable\n",
        "* https://medium.com/ax1al/a-brief-introduction-to-pe-format-6052914cc8dd\n",
        "* https://resources.infosecinstitute.com/topic/presenting-the-pe-header/\n",
        "\n",
        "\n",
        "Here are some highlights:\n",
        "\n",
        "The features are based on DOS_Header, FILE_HEADER AND OPTIONAL_HEADER. If any exception happens\n",
        "then the values will be assigned as zero for that header. Many PE files don't have DOS_Header then all the header will be assigned '0'.\n",
        "\n",
        "IMAGE_DOS_HEADER (19)  \n",
        "* \"e_magic\", \"e_cblp\", \"e_cp\",\"e_crlc\",\"e_cparhdr\",\n",
        "* \"e_minalloc\",\"e_maxalloc\",\"e_ss\",\"e_sp\",\n",
        "* \"e_csum\",\"e_ip\",\"e_cs\",\"e_lfarlc\",\"e_ovno\",\"e_res\",\n",
        "* \"e_oemid\",\"e_oeminfo\",\"e_res2\",\"e_lfanew\"\n",
        "\n",
        "FILE_HEADER (7)  \n",
        "* \"Machine\",\"NumberOfSections\",\"CreationYear\",\"PointerToSymbolTable\",\n",
        "* \"NumberOfSymbols\",\"SizeOfOptionalHeader\",\"Characteristics\"\n",
        "\n",
        "OPTIONAL_HEADER (29)  \n",
        "* \"Magic\",\"MajorLinkerVersion\",\"MinorLinkerVersion\",\"SizeOfCode\",\"SizeOfInitializedData\",\n",
        "* \"SizeOfUninitializedData\",\"AddressOfEntryPoint\",\n",
        "* \"BaseOfCode\",\"BaseOfData\",\"ImageBase\",\"SectionAlignment\",\"FileAlignment\",\n",
        "* \"MajorOperatingSystemVersion\",\"MinorOperatingSystemVersion\",\n",
        "* \"MajorImageVersion\", \"MinorImageVersion\", \"MajorSubsystemVersion\",\n",
        "* \"MinorSubsystemVersion\", \"SizeOfImage\", \"SizeOfHeaders\", \"CheckSum\",\n",
        "* \"Subsystem\", \"DllCharacteristics\", \"SizeOfStackReserve\", \"SizeOfStackCommit\",\n",
        "* \"SizeOfHeapReserve\", \"SizeOfHeapCommit\", \"LoaderFlags\", \"NumberOfRvaAndSizes\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Determining Feature Classifications\n",
        "\n",
        "Using the PE File information determine which features have no information to add (drop_features), which features relate to a numerical value (numerical_features) and finally which features relate to a category (categorical_features)\n",
        "\n",
        "\n",
        "Get a count of the features to drop, numerical_features and categorical features"
      ],
      "metadata": {
        "id": "NkeQTR7qHHeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drop_features = [\"e_magic\",\"e_crlc\",\"e_res\",\"e_res2\"]\n",
        "numerical_features = [\"e_cblp\",\"e_cp\",\"e_cparhdr\",\"e_minalloc\",\"e_maxalloc\",\"e_ss\",\"e_sp\",\"e_ip\",\"e_cs\",\"e_lfarlc\",\"e_ovno\",\"e_lfanew\",\n",
        "                      \"NumberOfSections\",\"CreationYear\",\"PointerToSymbolTable\",\"NumberOfSymbols\",\"SizeOfOptionalHeader\",\n",
        "                      \"SizeOfCode\",\"SizeOfInitializedData\",\"SizeOfUninitializedData\",\"AddressOfEntryPoint\",\"BaseOfCode\",\"BaseOfData\",\n",
        "                      \"ImageBase\",\"SectionAlignment\",\"FileAlignment\",\n",
        "                      \"SizeOfImage\",\"SizeOfHeaders\",\"SizeOfStackReserve\",\"SizeOfStackCommit\",\"SizeOfHeapReserve\",\n",
        "                      \"SizeOfHeapCommit\",\"NumberOfRvaAndSizes\"\n",
        "                      ]\n",
        "categorical_features = [\"e_csum\",\"e_oemid\",\"e_oeminfo\",\"Machine\",\"Characteristics\",\"Magic\",\"CheckSum\",\"DllCharacteristics\",\n",
        "                        \"LoaderFlags\",\"Subsystem\",\"MajorOperatingSystemVersion\",\n",
        "                      \"MinorOperatingSystemVersion\",\"MajorImageVersion\",\"MinorImageVersion\",\"MajorSubsystemVersion\",\"MinorSubsystemVersion\",\n",
        "                      \"MajorLinkerVersion\",\"MinorLinkerVersion\"]\n",
        "print(\"You have categorized {} of 55 features\".format(len(drop_features)+len(numerical_features)+len(categorical_features)))"
      ],
      "metadata": {
        "id": "MhaFekBuft5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drop_feature_count = len(drop_features)\n",
        "numerical_feature_count = len(numerical_features)\n",
        "categorical_feature_count = len(categorical_features)"
      ],
      "metadata": {
        "id": "mkj7NMLJHFFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n20E0iqfJlFy"
      },
      "source": [
        "----\n",
        "## Section 2: Preparing the Data\n",
        "Before data can be used as input for machine learning algorithms, it often must be cleaned, formatted, and restructured — this is typically known as **preprocessing**. Fortunately, for this dataset, there are no invalid or missing entries we must deal with, however, there are some qualities about certain features that must be adjusted. This preprocessing can help tremendously with the outcome and predictive power of nearly all learning algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** If you are interested in learning more about machine learning in general, or need an explanation of some of the concepts covered, a good resource is the Google Machine Learning Crash Course found at https://developers.google.com/machine-learning/crash-course"
      ],
      "metadata": {
        "id": "f5tH3jAltWrL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcAuAuKmJlFz"
      },
      "source": [
        "### Split the dataset between Features and Target before manipulation\n",
        "To create a machine learning model, the dataset will need to be split between the feature and target columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiqFH2AcJlFz"
      },
      "outputs": [],
      "source": [
        "# Split the data into features (dataframe with numerical and categorical features) and target label (pandas series with target column)\n",
        "Target = data[\"class\"]\n",
        "features_raw = data.drop(drop_features + [\"class\"], axis = 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j75VOGsoJlFz"
      },
      "source": [
        "### Normalizing Numerical Features\n",
        "In addition to performing transformations on features that are highly skewed, it is often good practice to perform some type of scaling on numerical features. Applying a scaling to the data does not change the shape of each feature's distribution ]however, normalization ensures that each feature is treated equally when applying supervised learners. Note that once scaling is applied, observing the data in its raw form will no longer have the same original meaning, as exampled below.\n",
        "\n",
        "Run the code cell below to normalize each numerical feature. We will use [`sklearn.preprocessing.MinMaxScaler`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) for this."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# Initialize a scaler, then apply it to the features\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# create a dataframe with the scaled values of the features and with the same column names they had before\n",
        "num_features_scaled = pd.DataFrame(scaler.fit_transform(features_raw[numerical_features]), columns = features_raw[numerical_features].columns)\n",
        "\n",
        "# Show an example of the first 5 record with scaling applied\n",
        "num_features_scaled.head()"
      ],
      "metadata": {
        "id": "wgH2Yzu4jhbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGejiZ7HJlFz"
      },
      "source": [
        "### Implementation: Data Preprocessing One Hot Encoding\n",
        "\n",
        "From the table in **Exploring the Data** above, we can see there are several features for each record that are non-numeric. Typically, learning algorithms expect input to be numeric, which requires that non-numeric features (called *categorical variables*) be converted. One popular way to convert categorical variables is by using the **one-hot encoding** scheme. One-hot encoding creates a _\"dummy\"_ variable for each possible category of each non-numeric feature. For example, assume `someFeature` has three possible entries: `A`, `B`, or `C`. We then encode this feature into `someFeature_A`, `someFeature_B` and `someFeature_C`.\n",
        "\n",
        "|   | someFeature |                    | someFeature_A | someFeature_B | someFeature_C |\n",
        "| :-: | :-: |                            | :-: | :-: | :-: |\n",
        "| 0 |  B  |  | 0 | 1 | 0 |\n",
        "| 1 |  C  | ----> one-hot encode ----> | 0 | 0 | 1 |\n",
        "| 2 |  A  |  | 1 | 0 | 0 |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat_features_raw = features_raw[categorical_features].copy()\n",
        "for col in categorical_features:\n",
        "  cat_features_raw[col] = cat_features_raw[col].astype(\"category\")"
      ],
      "metadata": {
        "id": "CbZN128plG7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Given what you know about the data do you think our dataset will gain any usability by one hot encoding columns? (return either \"yes\" or \"no\")\n",
        "one_hot_encoding = \"yes\"\n",
        "\n",
        "# If you answered yes then use pandas get_dummies function and drop the first category\n",
        "cat_features_final = pd.get_dummies(cat_features_raw, drop_first=True)\n",
        "\n",
        "# Print the number of categorical features after one-hot encoding\n",
        "num_features_encoded_all = cat_features_final.shape[1]\n",
        "print(\"{} total features after one-hot encoding.\".format(num_features_encoded_all))\n"
      ],
      "metadata": {
        "id": "CBU2VYpgjpFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note:\n",
        "Since we have a limited number of rows we need to be careful to not have too many features. Because of this we will be also dropping Checksum and all of its derived features. So make sure to remove these before continuing with the analysis. "
      ],
      "metadata": {
        "id": "9yk11YYz_5dJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select categorical features from the dataset and set them as a category\n",
        "cat_features_raw = features_raw[list(set(categorical_features)-set([\"CheckSum\"]))].copy()\n",
        "for col in list(set(categorical_features)-set([\"CheckSum\"])):\n",
        "  cat_features_raw[col] = cat_features_raw[col].astype(\"category\")\n",
        "\n",
        "# one hot encode the features and drop the first category\n",
        "cat_features_final = pd.get_dummies(cat_features_raw, drop_first=True)\n",
        "\n",
        "# Print the number of categorical features after one-hot encoding (with CheckSum removed)\n",
        "num_features_encoded_final = cat_features_final.shape[1]\n",
        "print(\"{} total features after one-hot encoding.\".format(num_features_encoded_final))"
      ],
      "metadata": {
        "id": "RLKrBLloB528"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Joining Data\n",
        "Finally we have our numeric and our categorical data so we are ready to join the columns together to create a final features dataset."
      ],
      "metadata": {
        "id": "KhIl3NTKCltZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DpQl-g0JlF0"
      },
      "outputs": [],
      "source": [
        "# This combines and cleans the data and removes any nan values\n",
        "features_final = pd.concat([num_features_scaled,cat_features_final],axis=1)\n",
        "\n",
        "features_final = features_final.replace((np.inf, -np.inf, np.nan), 0).reset_index(drop=True)\n",
        "display(features_final.head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRiLZToaJlF0"
      },
      "source": [
        "### Train/Test Dataset Split\n",
        "Now all _categorical variables_ have been converted into numerical features, and all numerical features have been normalized. \n",
        "Run the code cell below to perform this split."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import train_test_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the 'features' and 'Target' data into training and testing sets using a train set that is 80% of the data and a random state of np.random.RandomState(seed)\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_final, Target, test_size=0.2, random_state=np.random.RandomState(seed))\n",
        "\n",
        "n_train_records = X_train.shape[0]\n",
        "n_test_records = X_test.shape[0]\n",
        "\n",
        "# Show the results of the split\n",
        "print(\"Training set has {} samples.\".format(n_train_records))\n",
        "print(\"Testing set has {} samples.\".format(n_test_records))"
      ],
      "metadata": {
        "id": "7xwVkwb1jwi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "section_2 = {    \n",
        "    \"unique_data_types\": unique_data_types,\n",
        "    \"drop_feature_count\":drop_feature_count,\n",
        "    \"numerical_feature_count\":numerical_feature_count,\n",
        "    \"categorical_feature_count\":categorical_feature_count,\n",
        "    \"one_hot_encoding\": one_hot_encoding,\n",
        "    \"num_features_encoded_all\": num_features_encoded_all,\n",
        "    \"num_features_encoded_final\":num_features_encoded_final,\n",
        "    \"n_train_records\": n_train_records,\n",
        "    \"n_test_records\": n_test_records\n",
        "    }"
      ],
      "metadata": {
        "id": "oHdFXlyOj1ZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_sections = {**section_1,**section_2}\n",
        "with open('submission.json', 'w') as f:\n",
        "     f.write(json.dumps(all_sections))"
      ],
      "metadata": {
        "id": "rwgssSCxqwFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Section 3: Creating Models on the Data\n",
        "\n",
        "\n",
        "Now that the Dataset has been Preprocessed it is time to create Models using it. In our case we would like to know if a given row of the dataset is malware or not malware. This Modeling task is called Classification. We can use anything from Naive to Complex Models and in this project will only touch on a few different types of models. There are many additional classification model types but most will fit into similar training/analysis pattern."
      ],
      "metadata": {
        "id": "V5ItJ7-F8-dS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5amWygrJlF0"
      },
      "source": [
        "### Evaluate the Naive Predictor Performace\n",
        "* If we chose a model that always predicted a file was **NOT** malware, what would  that model's accuracy and F-score be on this dataset? \n",
        "\n",
        "**Please note** that the the purpose of generating a naive predictor is simply to show what a base model without any intelligence would look like. In the real world, ideally your base model would be either the results of a previous model or could be based on a research paper upon which you are looking to improve. When there is no benchmark model set, getting a result better than a basic assumption or a random choice is a place you could start from.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** To learn more about accuracy, recall, precision, and f-score, two sources of information are https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall and https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/"
      ],
      "metadata": {
        "id": "7_fDUv_7txNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "# you can read up more on each of these sklearn metrics here: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics \n",
        "\n",
        "#TODO: Calculate accuracy, precision, recall and F-Score for the Test Data assuming the model predicts every sample is NOT malware \n",
        "\n",
        "naive_accuracy = accuracy_score(y_test,[0]*len(y_test))\n",
        "naive_recall = recall_score(y_test,[0]*len(y_test))\n",
        "naive_precision = precision_score(y_test,[0]*len(y_test))\n",
        "naive_fscore = f1_score(y_test,[0]*len(y_test))\n",
        "\n",
        "# Print the results \n",
        "print(\"Naive Predictor: [Accuracy score: {:.4f}, Recall: {:.4f}, Precision: {:.4f}, F-score: {:.4f}]\".format(naive_accuracy, naive_recall, naive_precision, naive_fscore))"
      ],
      "metadata": {
        "id": "NPHCkiQyqGLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optional:** Create a Confusion Matrix plot to visualize the results \n",
        "* https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html"
      ],
      "metadata": {
        "id": "7LJfBokKzzpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "section_3 = {\n",
        "    \"naive_accuracy\": round(naive_accuracy,4),\n",
        "    \"naive_recall\": round(naive_recall,4),\n",
        "    \"naive_precision\": round(naive_precision,4),\n",
        "    \"naive_fscore\": round(naive_fscore,4),\n",
        "}"
      ],
      "metadata": {
        "id": "KzaBz2OkqGSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_sections = {**section_1,**section_2,**section_3}\n",
        "with open('submission.json', 'w') as f:\n",
        "     f.write(json.dumps(all_sections))"
      ],
      "metadata": {
        "id": "8d25nMnmzh37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Discussion Question 1: Explain accuracy, recall, precision and fscore and give an example of where they may be useful when analyzing classification results**"
      ],
      "metadata": {
        "id": "SwjGwPQZW2-u"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tJz8Mp6JlF1"
      },
      "source": [
        "### Section 3.1: Supervised Learning Models\n",
        "**The following are some of the supervised learning models that are currently available in** [`scikit-learn`](http://scikit-learn.org/stable/supervised_learning.html) **which is the easiest place to start when creating models:**\n",
        "- Gaussian Naive Bayes (GaussianNB)\n",
        "- Decision Trees\n",
        "- Ensemble Methods (Bagging, AdaBoost, Random Forest, Gradient Boosting)\n",
        "- K-Nearest Neighbors (KNeighbors)\n",
        "- Stochastic Gradient Descent Classifier (SGDC)\n",
        "- Support Vector Machines (SVM)\n",
        "- Logistic Regression\n",
        "\n",
        "We picked a few that we want you to initialize, train and use to predict on the test set. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** To learn more about supervised learning models, a useful resource is the scikit-learning documentation at https://scikit-learn.org/stable/supervised_learning.html. \n",
        "\n",
        "If you prefer video lectures, the freeCodeCamp Machine Learning in Python Tutorial at https://www.youtube.com/watch?v=pqNCD_5r0IU may be helpful."
      ],
      "metadata": {
        "id": "nfRM0LZouqNK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Section 3.1.1: Logistic Regression**\n",
        "\n",
        "*What is Logistic Regression?*\n",
        "\n",
        "This type of statistical analysis (also known as logit model) is often used for predictive analytics and modeling, and extends to applications in machine learning. In this analytics approach, the dependent variable is finite or categorical: either A or B (binary regression) or a range of finite options A, B, C or D (multinomial regression). It is used in statistical software to understand the relationship between the dependent variable and one or more independent variables by estimating probabilities using a logistic regression equation. \n",
        "\n",
        "This type of analysis can help you predict the likelihood of an event happening or a choice being made. For example, you may want to know the likelihood of a visitor choosing an offer made on your website — or not (dependent variable). Your analysis can look at known characteristics of visitors, such as sites they came from, repeat visits to your site, behavior on your site (independent variables). \n",
        "\n",
        "\n",
        "https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression"
      ],
      "metadata": {
        "id": "DEihe_hJ1th8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Initialize Model**\n",
        "use a random_state of np.random.RandomState(seed), maximum iterations of 1,000 and a L2 penalty term and make sure the regression converges"
      ],
      "metadata": {
        "id": "wno0FiXO2CTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression(random_state = np.random.RandomState(seed), penalty= 'l2', max_iter=1000)"
      ],
      "metadata": {
        "id": "6L2aYC5hqUNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Train Model**\n",
        "Use the training dataset you split in Section 2 (features and targets) to fit a model. Then predict on the test dataset you split in section 2 (features)"
      ],
      "metadata": {
        "id": "q3wCloqG2Gxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logreg.fit(X_train, y_train)\n",
        "preds_binary = logreg.predict(X_test)\n",
        "predicted_probas = logreg.predict_proba(X_test)"
      ],
      "metadata": {
        "id": "tnpwSFEl2J9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Analyze Model Results**\n",
        "Use the Test Dataset you split in Section 2 (features and targets) to Analyze the Model's Performance"
      ],
      "metadata": {
        "id": "QK_wum6f2KXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import auc,roc_curve\n",
        "logreg_accuracy = #TODO\n",
        "logreg_recall = #TODO\n",
        "logreg_precision = #TODO\n",
        "logreg_fscore = #TODO\n",
        "logreg_roc_auc = auc(fpr, tpr)"
      ],
      "metadata": {
        "id": "cmaHCJzC2MsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optional:** plot the Receiver Operating Characteristic Curve\n",
        "* https://en.wikipedia.org/wiki/Receiver_operating_characteristic"
      ],
      "metadata": {
        "id": "1feFZ3_51veh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feel free to play around with the model hyperparameters after you finish this project to understand better how they affect model performance"
      ],
      "metadata": {
        "id": "gaJo6372_wh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the top 5 features of the Logistic Regression Model using sklearn.feature_selection.RFE and answer the following:\n",
        "\n",
        "Identify the top 5 Features and the estimated importance (coefficient) to the model results (use absolute value of the coef to sort the order of the rank 1 features)\n",
        "1. **TODO Enter Top ranked Feature and Relative importance \n",
        "2. **TODO Enter Second ranked Feature and Relative importance \n",
        "3. **TODO Enter Third ranked Feature and Relative importance \n",
        "4. **TODO Enter fourth ranked Feature and Relative importance  \n",
        "5. **TODO Enter fifth ranked Feature and Relative importance "
      ],
      "metadata": {
        "id": "oXyIuLFL3Xxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "# Initialize RFE\n",
        "\n",
        "# Fit the RFE on the training data\n",
        "\n",
        "# sort and select the top features\n"
      ],
      "metadata": {
        "id": "M2zBzvjrwp8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logreg_top_feature_name = #TODO\n",
        "logreg_top_feature_relative_importance = #TODO\n",
        "logreg_second_feature_name = #TODO\n",
        "logreg_second_feature_relative_importance = #TODO\n",
        "logreg_third_feature_name = #TODO\n",
        "logreg_third_feature_relative_importance = #TODO\n",
        "logreg_fourth_feature_name = #TODO\n",
        "logreg_fourth_feature_relative_importance = #TODO\n",
        "logreg_fifth_feature_name = #TODO\n",
        "logreg_fifth_feature_relative_importance = #TODO"
      ],
      "metadata": {
        "id": "R6VuL6bi3VuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "section_3_1_1 = {\n",
        "    \"logreg_accuracy\": round(logreg_accuracy,4),\n",
        "    \"logreg_recall\": round(logreg_recall,4),\n",
        "    \"logreg_precision\": round(logreg_precision,4),\n",
        "    \"logreg_fscore\": round(logreg_fscore,4),\n",
        "    \"logreg_roc_auc\": round(logreg_roc_auc,4),\n",
        "    \"logreg_top_feature_name\" : logreg_top_feature_name,\n",
        "    \"logreg_top_feature_relative_importance\" : logreg_top_feature_relative_importance,\n",
        "    \"logreg_second_feature_name\" : logreg_second_feature_name,\n",
        "    \"logreg_second_feature_relative_importance\" : logreg_second_feature_relative_importance,\n",
        "    \"logreg_third_feature_name\" : logreg_third_feature_name,\n",
        "    \"logreg_third_feature_relative_importance\" : logreg_third_feature_relative_importance,\n",
        "    \"logreg_fourth_feature_name\" : logreg_fourth_feature_name,\n",
        "    \"logreg_fourth_feature_relative_importance\" : logreg_fourth_feature_relative_importance,\n",
        "    \"logreg_fifth_feature_name\" : logreg_fifth_feature_name,\n",
        "    \"logreg_fifth_feature_relative_importance\" : logreg_fifth_feature_relative_importance\n",
        "}"
      ],
      "metadata": {
        "id": "PmnW3p1UwwEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_sections = {**section_1,**section_2,**section_3,**section_3_1_1}\n",
        "with open('submission.json', 'w') as f:\n",
        "     f.write(json.dumps(all_sections))"
      ],
      "metadata": {
        "id": "ByDF264k1VH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Section 3.1.2: Random Forest**\n",
        "\n",
        "*What is Random forest?*\n",
        "\n",
        "The random forest algorithm is an extension of the bagging method as it utilizes both bagging and feature randomness to create an uncorrelated forest of decision trees. Feature randomness, also known as feature bagging or “the random subspace method”, generates a random subset of features, which ensures low correlation among decision trees. This is a key difference between decision trees and random forests. While decision trees consider all the possible feature splits, random forests only select a subset of those features.\n",
        "\n",
        "By accounting for all the potential variability in the data, we can reduce the risk of overfitting, bias, and overall variance, resulting in more precise predictions.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/ensemble.html#random-forests"
      ],
      "metadata": {
        "id": "7EJTE0BH-im7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Initialize Model**\n",
        "Please use the following Hyperparameters: \n",
        "- maximum features : sqrt\n",
        "- criterion : 'entropy'\n",
        "- random state : np.random.RandomState(seed)\n",
        "-number of estimators : 5000"
      ],
      "metadata": {
        "id": "vBAhyjpO-im_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "random_forest = #TODO"
      ],
      "metadata": {
        "id": "WDGwHx8Q-im_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Train Model**\n",
        "Use the training dataset you split in Section 2 (features and targets) to fit a model. Then predict on the test dataset you split in section 2 (features)"
      ],
      "metadata": {
        "id": "oToO5d94-im_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO"
      ],
      "metadata": {
        "id": "Fp5y8pPY-im_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Analyze Model Results**\n",
        "Use the Test Dataset you split in Section 2 (features and targets) to Analyze the Model's Performance"
      ],
      "metadata": {
        "id": "6-QG9-WA-im_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_forest_accuracy = #TODO\n",
        "random_forest_recall = #TODO\n",
        "random_forest_precision = #TODO\n",
        "random_forest_fscore = #TODO\n",
        "random_forest_roc_auc = #TODO"
      ],
      "metadata": {
        "id": "edYzN1WzzEEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the Feature Importance of the Model and answer the following Questions:\n",
        "\n",
        "Based on the Feature importance plot please identify the top 5 Features and the estimated importance to the model results\n",
        "1. **TODO Enter Top ranked Feature and Relative importance \n",
        "2. **TODO Enter Second ranked Feature and Relative importance \n",
        "3. **TODO Enter Third ranked Feature and Relative importance \n",
        "4. **TODO Enter fourth ranked Feature and Relative importance  \n",
        "5. **TODO Enter fifth ranked Feature and Relative importance "
      ],
      "metadata": {
        "id": "GjJPGhIf-im_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_forest_top_feature_name = #TODO\n",
        "random_forest_top_feature_relative_importance = #TODO\n",
        "random_forest_second_feature_name = #TODO\n",
        "random_forest_second_feature_relative_importance = #TODO\n",
        "random_forest_third_feature_name = #TODO\n",
        "random_forest_third_feature_relative_importance = #TODO\n",
        "random_forest_fourth_feature_name = #TODO\n",
        "random_forest_fourth_feature_relative_importance = #TODO\n",
        "random_forest_fifth_feature_name = #TODO\n",
        "random_forest_fifth_feature_relative_importance = #TODO"
      ],
      "metadata": {
        "id": "2sATnVi7-im_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feel free to play around with the hyperparameters after you finish this project to understand better how they affect model performance"
      ],
      "metadata": {
        "id": "ZEOxDAm5_vaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "section_3_1_2 = {\n",
        "    \"random_forest_accuracy\": round(random_forest_accuracy,4),\n",
        "    \"random_forest_recall\": round(random_forest_recall,4),\n",
        "    \"random_forest_precision\": round(random_forest_precision,4),\n",
        "    \"random_forest_fscore\": round(random_forest_fscore,4),\n",
        "    \"random_forest_roc_auc\": round(random_forest_roc_auc,4),\n",
        "    \"random_forest_top_feature_name\" : random_forest_top_feature_name,\n",
        "    \"random_forest_top_feature_relative_importance\" : random_forest_top_feature_relative_importance,\n",
        "    \"random_forest_second_feature_name\" : random_forest_second_feature_name,\n",
        "    \"random_forest_second_feature_relative_importance\" : random_forest_second_feature_relative_importance,\n",
        "    \"random_forest_third_feature_name\" : random_forest_third_feature_name,\n",
        "    \"random_forest_third_feature_relative_importance\" : random_forest_third_feature_relative_importance,\n",
        "    \"random_forest_fourth_feature_name\" : random_forest_fourth_feature_name,\n",
        "    \"random_forest_fourth_feature_relative_importance\" : random_forest_fourth_feature_relative_importance,\n",
        "    \"random_forest_fifth_feature_name\" : random_forest_fifth_feature_name,\n",
        "    \"random_forest_fifth_feature_relative_importance\" : random_forest_fifth_feature_relative_importance\n",
        "}"
      ],
      "metadata": {
        "id": "pVNmJxswza8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_sections = {**section_1, **section_2, **section_3, **section_3_1_1, **section_3_1_2}\n",
        "with open('submission.json', 'w') as f:\n",
        "     f.write(json.dumps(all_sections))"
      ],
      "metadata": {
        "id": "mbtbCJClKXCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Section 3.1.3: Gradient Boosting**\n",
        "\n",
        "*What is Gradient Boosting?*\n",
        "\n",
        "Gradient boosting is a technique used in creating models for prediction. The technique is mostly used in regression and classification procedures. Prediction models are often presented as decision trees for choosing the best prediction. Gradient boosting presents model building in stages, just like other boosting methods, while allowing the generalization and optimization of differentiable loss functions.\n"
      ],
      "metadata": {
        "id": "WYeNpZ0Ga2aV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Initialize Model**\n",
        "\n",
        "- random state : np.random.RandomState(seed)\n",
        "- learning rate : 0.1\n",
        "- max depth : 3\n",
        "- number of estimators : 100"
      ],
      "metadata": {
        "id": "QmfN828Aa2aZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "gradient = #TODO"
      ],
      "metadata": {
        "id": "CbMEzW9ta2aZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Train Model**\n",
        "Use the Train dataset you split in Section 2 (features and targets) to fit a model"
      ],
      "metadata": {
        "id": "xV48JMd_a2aZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO"
      ],
      "metadata": {
        "id": "BDfHMg3Xa2aZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Analyze Model Results**\n",
        "Use the Test Dataset you split in Section 2 (features and targets) to Analyze the Model's Performance"
      ],
      "metadata": {
        "id": "iedB_N3ca2aZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_accuracy = #TODO\n",
        "gradient_recall = #TODO\n",
        "gradient_precision = #TODO\n",
        "gradient_fscore = #TODO\n",
        "gradient_roc_auc = #TODO"
      ],
      "metadata": {
        "id": "YLnHcr0mz5ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the Feature Importance of the Model and answer the following Questions:\n",
        "\n",
        "Based on the Feature importance plot please identify the top 5 Features and the estimated importance to the model results\n",
        "1. **TODO Enter Top ranked Feature and Relative importance \n",
        "2. **TODO Enter Second ranked Feature and Relative importance \n",
        "3. **TODO Enter Third ranked Feature and Relative importance \n",
        "4. **TODO Enter fourth ranked Feature and Relative importance  \n",
        "5. **TODO Enter fifth ranked Feature and Relative importance "
      ],
      "metadata": {
        "id": "1-MrvSfNa2aZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feel free to play around with the hyperparameters after you finish this project to understand better how they affect model performance"
      ],
      "metadata": {
        "id": "PIvScEjs_iMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_top_feature_name = #TODO\n",
        "gradient_top_feature_relative_importance = #TODO\n",
        "gradient_second_feature_name = #TODO\n",
        "gradient_second_feature_relative_importance = #TODO\n",
        "gradient_third_feature_name = #TODO\n",
        "gradient_third_feature_relative_importance = #TODO\n",
        "gradient_fourth_feature_name = #TODO\n",
        "gradient_fourth_feature_relative_importance = #TODO\n",
        "gradient_fifth_feature_name = #TODO\n",
        "gradient_fifth_feature_relative_importance = #TODO"
      ],
      "metadata": {
        "id": "sPP8uVdI0Sn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "section_3_1_3 = {\n",
        "    \"gradient_accuracy\": gradient_accuracy,\n",
        "    \"gradient_recall\": gradient_recall,\n",
        "    \"gradient_precision\": gradient_precision,\n",
        "    \"gradient_fscore\": gradient_fscore,\n",
        "    \"gradient_roc_auc\": gradient_roc_auc,\n",
        "    \"gradient_top_feature_name\" : gradient_top_feature_name,\n",
        "    \"gradient_top_feature_relative_importance\" : gradient_top_feature_relative_importance,\n",
        "    \"gradient_second_feature_name\" : gradient_second_feature_name,\n",
        "    \"gradient_second_feature_relative_importance\" : gradient_second_feature_relative_importance,\n",
        "    \"gradient_third_feature_name\" : gradient_third_feature_name,\n",
        "    \"gradient_third_feature_relative_importance\" : gradient_third_feature_relative_importance,\n",
        "    \"gradient_fourth_feature_name\" : gradient_fourth_feature_name,\n",
        "    \"gradient_fourth_feature_relative_importance\" : gradient_fourth_feature_relative_importance,\n",
        "    \"gradient_fifth_feature_name\" : gradient_fifth_feature_name,\n",
        "    \"gradient_fifth_feature_relative_importance\" : gradient_fifth_feature_relative_importance\n",
        "}"
      ],
      "metadata": {
        "id": "Bnt37byN0SrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_sections = {**section_1, **section_2, **section_3, **section_3_1_1, **section_3_1_2, **section_3_1_3}\n",
        "with open('submission.json', 'w') as f:\n",
        "     f.write(json.dumps(all_sections))"
      ],
      "metadata": {
        "id": "-5t5ZSOKKTgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3LW6X57JlF4"
      },
      "source": [
        "#### **Discussion Question 2: Which was the best model you trained in Section 3.1??**\n",
        "Which was the best performing model: logistic regression, gradient boosting, random forest and why do you think this is the case?  \n",
        "**  TODO Provide your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DEdCRu9JlF4"
      },
      "source": [
        "### Section 3.2: Unsupervised Machine Learning\n",
        "\n",
        "The previous 3 sections we had features and targets and were trying to predict the value of something. With unsupervised learning we are just using the features to try to group them in some way or to gather some additional insights without actually predicting malware or not malware\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Section 3.2.1: Elbow method k means python**\n",
        "\n",
        "K-Means is an unsupervised machine learning algorithm that groups data into k number of clusters. The number of clusters is user-defined and the algorithm will try to group the data even if this number is not optimal for the specific case.\n",
        "\n",
        "Therefore we have to come up with a technique that somehow will help us decide how many clusters we should use for the K-Means model.\n",
        "\n",
        "The Elbow method is a very popular technique and the idea is to run k-means clustering for a range of clusters k (let’s say from 1 to 10) and for each value, we are calculating the sum of squared distances from each point to its assigned center(distortions).\n",
        "\n",
        "When the distortions are plotted and the plot looks like an arm then the “elbow”(the point of inflection on the curve) is the best value of k.\n",
        "    \n",
        "Use the following Parameters in Kmeans:\n",
        "\n",
        "kmeans_kwargs = {  \n",
        "   ...:     \"init\": \"random\",  \n",
        "   ...:     \"n_init\": 10,  \n",
        "   ...:     \"max_iter\": 300,  \n",
        "   ...:     \"random_state\": 0,  \n",
        "   ...: }  "
      ],
      "metadata": {
        "id": "V3Mo_Ps-dKlg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_aZqYqDJlF5"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "#TODO train a different kmeans model for each value of k using the train dataset and log the SSE score for each k\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMfROe8HJlF5"
      },
      "outputs": [],
      "source": [
        "# TODO Plot Cluster SSE scores"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional EDA:\n",
        "# Use the newly created clusters as an additional data point for your EDA plots "
      ],
      "metadata": {
        "id": "_SXsfyvgYvoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score \n",
        "kmeans_n_clusters = #TODO Determine the optimal value of k from ELBOW plot \n",
        "\n",
        "# fit kmeans model using this optimal value of k\n",
        "kmeans = #TODO\n",
        "\n",
        "\n",
        "# Predict the cluster for each data point in the train and test datasets\n",
        "\n",
        "train_preds = #TODO\n",
        "test_preds = #TODO\n",
        "\n",
        "# Given what you know about the data do you think our dataset will gain any usability by one hot encoding columns? \n",
        "# if so use the get_dummies without dropping the first feature this time\n",
        "# and Add pred clusters to new versions of the train and test datasets you should use data_frame.copy()\n",
        "# Name this cluster feature kmeans_cluster_id\n",
        "kmeans_one_hot_encoding = #TODO\n",
        "\n",
        "kmeans_train = #TODO\n",
        "kmeans_test = #TODO\n",
        "\n",
        "# Calculate the mean silhouette coefficient for the number of clusters chosen\n",
        "kmeans_optimal_sse_score = #TODO"
      ],
      "metadata": {
        "id": "w--aLNog02xU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "section_3_2_1 = {\n",
        "    \"kmeans_n_clusters\": kmeans_n_clusters,\n",
        "    \"kmeans_one_hot_encoding\":kmeans_one_hot_encoding,\n",
        "    \"kmeans_optimal_sse_score\": kmeans_optimal_sse_score\n",
        "}"
      ],
      "metadata": {
        "id": "1lsbdQaE020i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_sections = {**section_1, **section_2, **section_3, **section_3_1_1, **section_3_1_2, **section_3_1_3, **section_3_2_1}\n",
        "with open('submission.json', 'w') as f:\n",
        "     f.write(json.dumps(all_sections))"
      ],
      "metadata": {
        "id": "weLPd6Q8KOyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7ju7gQgJlF5"
      },
      "source": [
        "#### **Section 3.2.2: Principal Component Analysis(PCA)**\n",
        "What is Principal Component Analysis?\n",
        "\n",
        "PCA is a dimensionality reduction framework in machine learning. According to Wikipedia, PCA (or Principal Component Analysis) is a “statistical procedure that uses orthogonal transformation to convert a set of observations of possibly correlated variables…into a set of values of linearly uncorrelated variables called principal components.”\n",
        "\n",
        "The Benefits of PCA (Principal Component Analysis)\n",
        "PCA is an unsupervised learning technique that offers a number of benefits. For example, by reducing the dimensionality of the data, PCA enables us to better generalize machine learning models. This helps us deal with the “curse of dimensionality”.\n",
        "\n",
        "Most, if not all, algorithm performance depends on the dimension of the data. Models running on very high dimensional data might perform very slow—or even fail—and require significant server resources. PCA can help us improve performance at a very low cost of model accuracy. \n",
        "\n",
        "Other benefits of PCA include reduction of noise in the data, feature selection (to a certain extent), and the ability to produce independent, uncorrelated features of the data. PCA also allows us to visualize data and allow for the inspection of clustering/classification algorithms. \n",
        "\n",
        "**Directions:** please create a PCA with 5 features and set the randomness with your seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyZX6SBuJlF5"
      },
      "outputs": [],
      "source": [
        "#Implement PCA\n",
        "from sklearn.decomposition import PCA\n",
        "pca = #TODO Implement PCA with 5 Components\n",
        "\n",
        "# fit PCA using the training dataset and transform the train and test sets\n",
        "\n",
        "principalComponents_train = \n",
        "principalComponents_test = \n",
        "# Create new Df with PCA Features generated from the train dataset you created in section 2\n",
        "principalDf_train = pd.DataFrame(data = principalComponents_train\n",
        "             , columns = ['principal component 1', 'principal component 2','principal component 3', 'principal component 4','principal component 5'])\n",
        "principalDf_test = pd.DataFrame(data = principalComponents_test\n",
        "             , columns = ['principal component 1', 'principal component 2','principal component 3', 'principal component 4','principal component 5'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daZZUOO7JlF5"
      },
      "outputs": [],
      "source": [
        "#TODO Find the Percentage of variance explained by each of the selected components\n",
        "pca_explainted_variance_ratio_component_1 = #TODO\n",
        "pca_explainted_variance_ratio_component_2 = #TODO\n",
        "pca_explainted_variance_ratio_component_3 = #TODO\n",
        "pca_explainted_variance_ratio_component_4 = #TODO\n",
        "pca_explainted_variance_ratio_component_5 = #TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the following Parameters in Kmeans:\n",
        "\n",
        "kmeans_kwargs = {\n",
        "...: \"init\": \"random\",\n",
        "...: \"n_init\": 10,\n",
        "...: \"max_iter\": 300,\n",
        "...: \"random_state\": 0,\n",
        "...: }"
      ],
      "metadata": {
        "id": "ocd0B_L1avb7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmMXuaZlJlF5"
      },
      "outputs": [],
      "source": [
        "#TODO train a different kmeans model for each value of k using the PCA dataset and log the SSE score for each k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SOS6i-JJlF6"
      },
      "outputs": [],
      "source": [
        "# TODO Plot Cluster SSE scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npLAADAbJlF6"
      },
      "outputs": [],
      "source": [
        "# Optional EDA:\n",
        "# Use the newly created clusters as an additional data point for your EDA plots "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pca_n_clusters = #TODO Determine the optimal value of k from ELBOW plot \n",
        "\n",
        "# fit kmeans model using this optimal value of k\n",
        "kmeans = KMeans(n_clusters = n, init=\"random\",n_init=10,max_iter=300,random_state=0)\n",
        "kmeans.fit(principalDf_train)\n",
        "# Predict the cluster for each data point in the train and test datasets\n",
        "train_preds = kmeans.predict(principalDf_train)\n",
        "test_preds = kmeans.predict(principalDf_test)\n",
        "\n",
        "# Given what you know about the data do you think our dataset will gain any usability by one hot encoding columns? (\"yes\",\"no\")\n",
        "pca_one_hot_encoding = #TODO\n",
        "\n",
        "# if so use the get_dummies without dropping the first feature this time\n",
        "# and Add pred clusters to new versions of the train and test datasets you should use data_frame.copy()\n",
        "# Name this cluster feature pca_kmeans_cluster_id\n",
        "\n",
        "#Add pred clusters to train and test datasets\n",
        "# Name this cluster feature pca_kmeans_cluster_id\n",
        "\n",
        "pca_kmeans_train = #TODO\n",
        "pca_kmeans_test = #TODO\n",
        "\n",
        "# Calculate the mean silhouette coefficient for the number of clusters chosen\n",
        "pca_optimal_sse_score = #TODO"
      ],
      "metadata": {
        "id": "ZGcqDobA3lcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "section_3_2_2 = {\n",
        "    \"pca_explainted_variance_ratio_component_1\":round(pca_explainted_variance_ratio_component_1,4),\n",
        "    \"pca_explainted_variance_ratio_component_2\":round(pca_explainted_variance_ratio_component_2,4),\n",
        "    \"pca_explainted_variance_ratio_component_3\":round(pca_explainted_variance_ratio_component_3,4),\n",
        "    \"pca_explainted_variance_ratio_component_4\":round(pca_explainted_variance_ratio_component_4,4),\n",
        "    \"pca_explainted_variance_ratio_component_5\":round(pca_explainted_variance_ratio_component_5,4),\n",
        "    \"pca_one_hot_encoding\":pca_one_hot_encoding,\n",
        "    \"pca_n_clusters\": pca_n_clusters,\n",
        "    \"pca_optimal_sse_score\": pca_optimal_sse_score\n",
        "}"
      ],
      "metadata": {
        "id": "uOkEk8is1hid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_sections = {**section_1, **section_2, **section_3, **section_3_1_1, **section_3_1_2, **section_3_1_3, **section_3_2_1,**section_3_2_2}\n",
        "with open('submission.json', 'w') as f:\n",
        "     f.write(json.dumps(all_sections))"
      ],
      "metadata": {
        "id": "MuRNaPBQKMEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geXQ8-VPJlF6"
      },
      "source": [
        "### Section 3.3: Supervised and Unsupervised Models combined  \n",
        "Next you will combine the Unsupervised and Supervised Machine Learning to predict the target Malware analysis with each of the two datasets you generated above (in the k-means and PCA sections)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Section 3.3.1: K-means + Gradient Boosting**\n",
        "\n",
        "Now use the K-means feature dataframe you generated for the k-means Clustering to train and test a Gradient Boosting model. Use the additional cluster feature as training/testing features and the (untouched) labels \n",
        "\n"
      ],
      "metadata": {
        "id": "wwuJK1F1enRe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Initialize Model**\n",
        "\n",
        "- random state : np.random.RandomState(seed)\n",
        "- learning rate : 0.1\n",
        "- max depth : 3\n",
        "- number of estimators : 100"
      ],
      "metadata": {
        "id": "I1aG8_FzenRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_kmeans = #TODO"
      ],
      "metadata": {
        "id": "OK1Twn1kenRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Train Model**\n",
        "Use the Train dataset features you created in section 3.2.1 and the targets you split in Section 2 to fit a model"
      ],
      "metadata": {
        "id": "Upugopl6enRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO"
      ],
      "metadata": {
        "id": "FEGRno_9enRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Analyze Model Results**\n",
        "Use the Test Dataset features you created in section 3.2.1 and the targets you split in Section 2 to Analyze the Model's Performance"
      ],
      "metadata": {
        "id": "dbY6mJ2lenRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_kmeans_accuracy = #TODO\n",
        "gradient_kmeans_recall = #TODO\n",
        "gradient_kmeans_precision = #TODO\n",
        "gradient_kmeans_fscore = #TODO\n",
        "gradient_kmeans_roc_auc = #TODO"
      ],
      "metadata": {
        "id": "5b-MgPfS1zcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the Feature Importance of the Model and answer the following Questions:\n",
        "\n",
        "Based on the Feature importance plot please identify the top 5 Features and the estimated importance to the model results\n",
        "1. **TODO Enter Top ranked Feature and Relative importance \n",
        "2. **TODO Enter Second ranked Feature and Relative importance \n",
        "3. **TODO Enter Third ranked Feature and Relative importance \n",
        "4. **TODO Enter fourth ranked Feature and Relative importance  \n",
        "5. **TODO Enter fifth ranked Feature and Relative importance "
      ],
      "metadata": {
        "id": "vbc9dvGTenRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_kmeans_top_feature_name = #TODO\n",
        "gradient_kmeans_top_feature_relative_importance = #TODO\n",
        "gradient_kmeans_second_feature_name = #TODO\n",
        "gradient_kmeans_second_feature_relative_importance = #TODO\n",
        "gradient_kmeans_third_feature_name = #TODO\n",
        "gradient_kmeans_third_feature_relative_importance = #TODO\n",
        "gradient_kmeans_fourth_feature_name = #TODO\n",
        "gradient_kmeans_fourth_feature_relative_importance = #TODO\n",
        "gradient_kmeans_fifth_feature_name = #TODO\n",
        "gradient_kmeans_fifth_feature_relative_importance = #TODO"
      ],
      "metadata": {
        "id": "7ccLfAEb1-Pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "section_3_3_1 = {\n",
        "    \"gradient_kmeans_accuracy\": round(gradient_kmeans_accuracy,4),\n",
        "    \"gradient_kmeans_recall\": round(gradient_kmeans_recall,4),\n",
        "    \"gradient_kmeans_precision\": round(gradient_kmeans_precision,4),\n",
        "    \"gradient_kmeans_fscore\": round(gradient_kmeans_fscore,4),\n",
        "    \"gradient_kmeans_roc_auc\": round(gradient_kmeans_roc_auc,4),\n",
        "    \"gradient_kmeans_top_feature_name\" : gradient_kmeans_top_feature_name,\n",
        "    \"gradient_kmeans_top_feature_relative_importance\" : gradient_kmeans_top_feature_relative_importance,\n",
        "    \"gradient_kmeans_second_feature_name\" : gradient_kmeans_second_feature_name,\n",
        "    \"gradient_kmeans_second_feature_relative_importance\" : gradient_kmeans_second_feature_relative_importance,\n",
        "    \"gradient_kmeans_third_feature_name\" : gradient_kmeans_third_feature_name,\n",
        "    \"gradient_kmeans_third_feature_relative_importance\" : gradient_kmeans_third_feature_relative_importance,\n",
        "    \"gradient_kmeans_fourth_feature_name\" : gradient_kmeans_fourth_feature_name,\n",
        "    \"gradient_kmeans_fourth_feature_relative_importance\" : gradient_kmeans_fourth_feature_relative_importance,\n",
        "    \"gradient_kmeans_fifth_feature_name\" : gradient_kmeans_fifth_feature_name,\n",
        "    \"gradient_kmeans_fifth_feature_relative_importance\" : gradient_kmeans_fifth_feature_relative_importance\n",
        "}"
      ],
      "metadata": {
        "id": "tOwpHZ2A1-V5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_sections = {**section_1, **section_2, **section_3, **section_3_1_1, **section_3_1_2, **section_3_1_3, **section_3_2_1,**section_3_2_2,**section_3_3_1}\n",
        "with open('submission.json', 'w') as f:\n",
        "     f.write(json.dumps(all_sections))"
      ],
      "metadata": {
        "id": "pDwut5hKKJVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Section 3.3.2: PCA Features + Gradient Boosting**\n",
        "\n",
        "Now use the PCA feature dataframe you generated for the PCA Clustering to train and test a Gradient Boosting model. Use the PCA features as training/testing features and the (untouched) labels \n",
        "\n",
        "**(Make sure you didnt run PCA on the labels(class) and only ran it on the features!!)**\n"
      ],
      "metadata": {
        "id": "u642epp1ba29"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Initialize Model**\n",
        "\n",
        "- random state : np.random.RandomState(seed)\n",
        "- learning rate : 0.1\n",
        "- max depth : 3\n",
        "- number of estimators : 100"
      ],
      "metadata": {
        "id": "uOZYHFNkba29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_pca = #TODO"
      ],
      "metadata": {
        "id": "JhUHpX14ba2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Train Model**\n",
        "Use the Train dataset features you created in section 3.2.2 and the targets you split in Section 2 to fit a model"
      ],
      "metadata": {
        "id": "u8YprtXeba2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO"
      ],
      "metadata": {
        "id": "cnaBPQD2ba2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Analyze Model Results**\n",
        "Use the Test Dataset features you created in section 3.2.2 and the targets you split in Section 2 to Analyze the Model's Performance"
      ],
      "metadata": {
        "id": "iz6AfShaba2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_pca_accuracy = #TODO\n",
        "gradient_pca_recall = #TODO\n",
        "gradient_pca_precision = #TODO\n",
        "gradient_pca_fscore = #TODO\n",
        "gradient_pca_roc_auc = #TODO"
      ],
      "metadata": {
        "id": "lahAUZKl24Xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the Feature Importance of the Model and answer the following Questions:\n",
        "\n",
        "Based on the Feature importance plot please identify the top 5 Features and the estimated importance to the model results\n",
        "1. **TODO Enter Top ranked Feature and Relative importance \n",
        "2. **TODO Enter Second ranked Feature and Relative importance \n",
        "3. **TODO Enter Third ranked Feature and Relative importance \n",
        "4. **TODO Enter fourth ranked Feature and Relative importance  \n",
        "5. **TODO Enter fifth ranked Feature and Relative importance "
      ],
      "metadata": {
        "id": "011Un65Zba2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_pca_top_feature_name = #TODO\n",
        "gradient_pca_top_feature_relative_importance = #TODO\n",
        "gradient_pca_second_feature_name = #TODO\n",
        "gradient_pca_second_feature_relative_importance = #TODO\n",
        "gradient_pca_third_feature_name = #TODO\n",
        "gradient_pca_third_feature_relative_importance = #TODO\n",
        "gradient_pca_fourth_feature_name = #TODO\n",
        "gradient_pca_fourth_feature_relative_importance = #TODO\n",
        "gradient_pca_fifth_feature_name = #TODO\n",
        "gradient_pca_fifth_feature_relative_importance = #TODO"
      ],
      "metadata": {
        "id": "7aOgHSiy37mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Discussion Question 3: Were the cluster_ids in your top 5 features?**\n",
        "Why do you think this was the case? Is there something about what PCA does that would make the kmeans clustering less relevant?\n",
        "  \n",
        "** TODO Provide your answer here"
      ],
      "metadata": {
        "id": "JgQNgOI9D7Q_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "section_3_3_2 = {\n",
        "    \"gradient_pca_accuracy\": round(gradient_pca_accuracy,4),\n",
        "    \"gradient_pca_recall\": round(gradient_pca_recall,4),\n",
        "    \"gradient_pca_precision\": round(gradient_pca_precision,4),\n",
        "    \"gradient_pca_fscore\": round(gradient_pca_fscore,4),\n",
        "    \"gradient_pca_roc_auc\": round(gradient_pca_roc_auc,4),\n",
        "    \"gradient_pca_top_feature_name\" : gradient_pca_top_feature_name,\n",
        "    \"gradient_pca_top_feature_relative_importance\" : gradient_pca_top_feature_relative_importance,\n",
        "    \"gradient_pca_second_feature_name\" : gradient_pca_second_feature_name,\n",
        "    \"gradient_pca_second_feature_relative_importance\" : gradient_pca_second_feature_relative_importance,\n",
        "    \"gradient_pca_third_feature_name\" : gradient_pca_third_feature_name,\n",
        "    \"gradient_pca_third_feature_relative_importance\" : gradient_pca_third_feature_relative_importance,\n",
        "    \"gradient_pca_fourth_feature_name\" : gradient_pca_fourth_feature_name,\n",
        "    \"gradient_pca_fourth_feature_relative_importance\" : gradient_pca_fourth_feature_relative_importance,\n",
        "    \"gradient_pca_fifth_feature_name\" : gradient_pca_fifth_feature_name,\n",
        "    \"gradient_pca_fifth_feature_relative_importance\" : gradient_pca_fifth_feature_relative_importance\n",
        "}"
      ],
      "metadata": {
        "id": "npeofspQ4FNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5ixgptvJlF7"
      },
      "source": [
        "#### **Discussion Question 4: Which was the best model you trained in Section 3.3??**\n",
        "Did this model outperform the supervised logistic regression, gradient boosting and random forest models you trained in Section 3.1 and why do you think it under or over performed compared to those models?    \n",
        "** TODO Provide your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combine JSON's into Submission file"
      ],
      "metadata": {
        "id": "gqJ1yNfu7Kgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_sections = {**section_1, **section_2, **section_3, **section_3_1_1, **section_3_1_2, **section_3_1_3, **section_3_2_1,**section_3_2_2,**section_3_3_1, **section_3_3_2}\n",
        "with open('submission.json', 'w') as f:\n",
        "     f.write(json.dumps(all_sections))"
      ],
      "metadata": {
        "id": "abz2W9yD7J4z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "w3LW6X57JlF4"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}